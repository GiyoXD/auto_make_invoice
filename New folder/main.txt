# --- START OF FULL FILE: main.py ---

import logging
import pprint # For nicely printing dictionaries
import re     # Keep import re, potentially used internally or future use
import decimal # Import decimal for type hints and potential direct use
from typing import Dict, List, Any, Optional, Tuple # Import necessary types

# Import from our refactored modules
import config as cfg # Use alias for clarity
from excel_handler import ExcelHandler
import sheet_parser
import data_processor # Includes all processing functions

# Configure logging (Set to DEBUG for detailed investigation)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s') # USE DEBUG

# --- Constants for Log Truncation ---
MAX_LOG_DICT_LEN = 3000 # Max length for printing large dicts in logs

def run_invoice_automation():
    """Main function to find tables, extract, and process data for each."""
    logging.info("--- Starting Invoice Automation ---")
    handler = None  # Initialize handler outside try block
    # Dictionary to store the final processed data (post-distribution) per table
    processed_tables: Dict[int, Dict[str, Any]] = {}
    # Dictionary to store the aggregated SQFT results per table
    all_aggregated_sqft_results: Dict[int, Dict[Tuple[Any, Any, Optional[decimal.Decimal]], decimal.Decimal]] = {}
    # Dictionary to store raw extracted data per table
    all_tables_data: Dict[int, Dict[str, List[Any]]] = {}

    try:
        # --- 1. Load Excel Sheet ---
        logging.info(f"Loading workbook: {cfg.INPUT_EXCEL_FILE}")
        handler = ExcelHandler(cfg.INPUT_EXCEL_FILE)
        sheet = handler.load_sheet(sheet_name=cfg.SHEET_NAME, data_only=True) # data_only=True is crucial

        if sheet is None:
            # Error already logged by ExcelHandler
            raise RuntimeError(f"Failed to load sheet from '{cfg.INPUT_EXCEL_FILE}'. Exiting.")

        # --- 2. Find All Header Rows ---
        logging.info("Searching for all header rows...")
        header_rows = sheet_parser.find_all_header_rows(
            sheet,
            cfg.HEADER_IDENTIFICATION_PATTERN,
            cfg.HEADER_SEARCH_ROW_RANGE,
            cfg.HEADER_SEARCH_COL_RANGE
        )

        if not header_rows:
             # Error/Warning already logged by find_all_header_rows
             raise RuntimeError("Could not find any header rows matching the pattern. Cannot proceed.")

        logging.info(f"Found {len(header_rows)} potential header row(s) at: {header_rows}") # Logged in find_all_header_rows too

        # --- 3. Map Columns (Based on the FIRST header) ---
        # Assumption: All tables below the first header use the same column layout
        first_header_row = header_rows[0]
        logging.info(f"Mapping columns based on the first identified header row ({first_header_row})...")
        column_mapping = sheet_parser.map_columns_to_headers(
            sheet,
            first_header_row,
            cfg.HEADER_SEARCH_COL_RANGE
        )

        if not column_mapping:
             # Error/Warning logged by map_columns_to_headers
             raise RuntimeError("Failed to map required columns from the first header row. Check header names and config.")
        # Log the mapping details at DEBUG level as it can be verbose
        logging.debug(f"Successfully mapped {len(column_mapping)} columns:")
        logging.debug(pprint.pformat(column_mapping))
        # Essential columns check log is already inside map_columns_to_headers

        # --- 4. Extract Data for All Tables ---
        logging.info("Extracting data for all identified tables using the derived column mapping...")
        all_tables_data = sheet_parser.extract_multiple_tables(
            sheet,
            header_rows,      # Pass the list of all found header rows
            column_mapping    # Pass the single mapping derived from the first header
        )

        # --- Log Raw Extracted Data (using DEBUG level for full detail) --- #
        logging.debug(f"--- Raw Extracted Data ({len(all_tables_data)} Table(s)) ---")
        if all_tables_data:
            logging.debug(f"\n{pprint.pformat(all_tables_data)}") # Log full dict at DEBUG
        else:
            logging.info("No raw data was extracted (dictionary is empty).") # INFO level if empty
        #######################################################################

        if not all_tables_data:
             # This might be normal if the sheet has headers but no data rows below them
             logging.warning("Extraction resulted in an empty data structure. Processing will stop. Check if sheet has data below headers or if extraction criteria (stop column, limits) are too strict.")
             # Stop processing if extraction yields nothing
             # return # Or raise specific exception


        # --- 5. Process Each Table Individually ---
        logging.info(f"--- Starting Data Processing Loop for {len(all_tables_data)} Extracted Table(s) ---")
        for table_index, raw_data_dict in all_tables_data.items():
            # Use .get() in case all_tables_data somehow doesn't have expected index (shouldn't happen)
            current_table_data = all_tables_data.get(table_index)
            if current_table_data is None:
                 logging.error(f"!!! Internal Error: Attempting to process table_index {table_index} but it's missing from all_tables_data. Skipping.")
                 continue

            logging.info(f"--- Processing Table Index {table_index} ---")
            logging.debug(f"Initial data for Table {table_index} (first 5 rows sample):\n{pprint.pformat({k: v[:5] for k, v in current_table_data.items()})}")


            # Basic check if the dictionary contains any actual data lists or rows
            if not isinstance(current_table_data, dict) or not current_table_data or not any(isinstance(v, list) and v for v in current_table_data.values()):
                logging.warning(f"Table {table_index} appears to be empty or contains no valid list data after extraction. Skipping CBM, Distribution, and Aggregation.")
                processed_tables[table_index] = current_table_data # Store the empty/invalid dict
                all_aggregated_sqft_results[table_index] = {} # Store empty aggregate result
                continue

            # --- 5a. Pre-process: Calculate CBM ---
            logging.info(f"Table {table_index}: Calculating CBM values...")
            try:
                 # process_cbm_column modifies the dictionary in place
                 # Pass the dictionary for the current table index
                 data_after_cbm = data_processor.process_cbm_column(current_table_data) # Modifies current_table_data
            except Exception as cbm_e:
                 logging.error(f"Error during CBM calculation for Table {table_index}: {cbm_e}", exc_info=True)
                 # The 'current_table_data' dictionary might be partially modified or unchanged
                 data_after_cbm = current_table_data # Use the dictionary state after the error attempt
                 logging.warning(f"Proceeding for Table {table_index} using data state after CBM calculation attempt (might be inconsistent if error occurred mid-process).")

            # --- 5b. Process: Distribute Values ---
            logging.info(f"Table {table_index}: Distributing values...")
            try:
                # distribute_values also modifies the dictionary in place
                # Use the dictionary that resulted from the CBM step
                data_after_distribution = data_processor.distribute_values(
                    data_after_cbm,             # Input dict (potentially modified by CBM)
                    cfg.COLUMNS_TO_DISTRIBUTE,  # List of columns like ["net", "gross", "cbm"]
                    cfg.DISTRIBUTION_BASIS_COLUMN # e.g., "pcs"
                )
                # Store this dictionary state as the main processed result for the table
                processed_tables[table_index] = data_after_distribution # This IS the modified data_after_cbm dict

            except data_processor.ProcessingError as pe:
                 # Handle specific processing errors from distribute_values
                 logging.error(f"Value distribution failed for Table {table_index}: {pe}. Storing data state *before* distribution attempt.")
                 # Store the data as it was *before* the failed distribution call
                 processed_tables[table_index] = data_after_cbm # Fallback to pre-distribution state
                 logging.warning(f"Skipping SQFT aggregation for Table {table_index} due to distribution error.")
                 all_aggregated_sqft_results[table_index] = {} # Ensure empty result stored
                 continue # Move to the next table index
            except Exception as dist_e:
                # Catch any other unexpected errors during distribution
                logging.error(f"An unexpected error occurred during value distribution for Table {table_index}: {dist_e}", exc_info=True)
                processed_tables[table_index] = data_after_cbm # Fallback state
                logging.warning(f"Skipping SQFT aggregation for Table {table_index} due to unexpected distribution error.")
                all_aggregated_sqft_results[table_index] = {}
                continue # Move to the next table index


            # --- 5c. Process: Aggregate SQFT ---
            # This step operates on the data *after* successful distribution
            logging.info(f"Table {table_index}: Aggregating SQFT...")
            try:
                # Get the data that was successfully processed through distribution
                # This should be the dictionary stored in processed_tables for this index
                data_for_aggregation = processed_tables.get(table_index)

                # --- DETAILED LOGGING BEFORE AGGREGATION CALL --- #############
                logging.debug(f"[main - Aggregation Input] --- Data being passed to aggregate_sqft_by_po_item_price for Table {table_index} ---")
                if data_for_aggregation and isinstance(data_for_aggregation, dict):
                    # Check for required keys specifically
                    required_keys = {'po', 'item', 'unit', 'sqft'}
                    keys_present = set(data_for_aggregation.keys())
                    required_keys_present = required_keys.issubset(keys_present)
                    logging.debug(f"[main - Aggregation Input] Required keys {required_keys} present? {required_keys_present}")
                    if not required_keys_present:
                        logging.warning(f"[main - Aggregation Input] Missing one or more required keys for SQFT aggregation in data for Table {table_index}. Keys present: {list(keys_present)}. Missing: {required_keys - keys_present}")

                    # Log the actual data using pprint (will be verbose, relying on DEBUG level)
                    logging.debug(f"[main - Aggregation Input] Content for Table {table_index}:\n{pprint.pformat(data_for_aggregation)}")
                elif not data_for_aggregation:
                    logging.warning(f"[main - Aggregation Input] Data dictionary for Table {table_index} is empty or None before aggregation call.")
                else:
                    logging.error(f"[main - Aggregation Input] Data for Table {table_index} is not a dictionary (Type: {type(data_for_aggregation).__name__}) before aggregation call.")
                    data_for_aggregation = {} # Ensure it's a dict for the call
                # --- END OF DETAILED LOGGING --- ##############################


                # Call the aggregation function only if data is a valid dict
                if isinstance(data_for_aggregation, dict):
                    aggregated_sqft_data = data_processor.aggregate_sqft_by_po_item_price(data_for_aggregation)
                else:
                     aggregated_sqft_data = {} # Cannot aggregate non-dict data

                # Store the aggregation results specific to this table
                all_aggregated_sqft_results[table_index] = aggregated_sqft_data

                # Log the aggregated results clearly for this table (only if not empty)
                if aggregated_sqft_data:
                    logging.info(f"--- Aggregated SQFT Results Summary for Table {table_index} ---")
                    logging.info(f"Unique (PO, Item, Price) combinations found: {len(aggregated_sqft_data)}")
                    # Log full details at DEBUG level
                    logging.debug(f"Full Aggregated SQFT Data for Table {table_index}:\n{pprint.pformat(aggregated_sqft_data)}")
                else:
                    logging.info(f"Table {table_index}: No SQFT data aggregated (result dictionary is empty). Check input data and aggregation logs.")

            except Exception as agg_e:
                 # Catch potential errors during aggregation for this specific table
                 logging.error(f"SQFT aggregation step failed unexpectedly for Table {table_index}: {agg_e}", exc_info=True)
                 # Ensure a placeholder is stored if aggregation fails
                 all_aggregated_sqft_results[table_index] = {}


            logging.info(f"--- Finished Processing All Steps for Table Index {table_index} ---")


        # --- 6. Output / Further Steps ---
        logging.info("--- All Table Processing Loops Completed ---")
        logging.info(f"Final processed data structure (post-distribution) contains results for {len(processed_tables)} table index(es): {list(processed_tables.keys())}")
        logging.info(f"Final aggregated SQFT results structure contains results for {len(all_aggregated_sqft_results)} table index(es): {list(all_aggregated_sqft_results.keys())}")

        # --- Log Final Processed Data (Post-Distribution) --- #
        logging.debug("--- Final Processed Data (Post-Distribution, All Tables) ---")
        if processed_tables:
            processed_output = pprint.pformat(processed_tables)
            # Log full output at DEBUG, potentially truncated at INFO if needed later
            logging.debug(f"\n{processed_output}")
        else:
             logging.info("No final processed data was generated (dictionary is empty).")
        ########################################################

        # --- Log Final Aggregated SQFT Results --- ##############
        logging.info("--- Final Aggregated SQFT Results (All Tables) ---")
        if all_aggregated_sqft_results:
            agg_sqft_output = pprint.pformat(all_aggregated_sqft_results)
            # Log full output at DEBUG, truncate for INFO if it were enabled
            if len(agg_sqft_output) > MAX_LOG_DICT_LEN and logging.getLogger().getEffectiveLevel() > logging.DEBUG:
                 agg_sqft_output = agg_sqft_output[:MAX_LOG_DICT_LEN] + "\n... (output truncated)"
            logging.info(f"\n{agg_sqft_output}") # Print potentially truncated at INFO
            logging.debug(f"Full Final Aggregated SQFT:\n{pprint.pformat(all_aggregated_sqft_results)}") # Full at DEBUG
        else:
            logging.info("No aggregated SQFT results were generated.")
        ########################################################


        # --- Future steps example: Save results ---
        # import pickle
        # output_file = "processed_invoice_data.pkl"
        # try:
        #     combined_results = {
        #         "extracted_data": all_tables_data, # Maybe save raw too
        #         "processed_tables": processed_tables,
        #         "aggregated_sqft": all_aggregated_sqft_results
        #     }
        #     with open(output_file, 'wb') as f_pickle:
        #         pickle.dump(combined_results, f_pickle)
        #     logging.info(f"Successfully saved combined processed data to {output_file}")
        # except Exception as pickle_e:
        #     logging.error(f"Failed to save data to pickle file {output_file}: {pickle_e}")

        logging.info("--- Invoice Automation Finished Successfully ---")

    except FileNotFoundError as e:
        logging.error(f"Input file error: {e}")
        # Potentially add specific user guidance here
    except RuntimeError as e:
        # Catch specific errors raised for flow control (no headers, mapping fail, etc.)
        logging.error(f"Processing halted due to critical error: {e}")
    except Exception as e:
        # Catch any other unexpected errors during setup or processing flow
        logging.error(f"An unexpected error occurred in the main script execution: {e}", exc_info=True)
    finally:
        # --- Clean up ---
        if handler:
            handler.close() # Release workbook reference
        logging.info("--- Automation Run Complete ---")


if __name__ == "__main__":
    run_invoice_automation()

# --- END OF FULL FILE: main.py ---